{"id":1276,"date":"2024-11-05T19:50:17","date_gmt":"2024-11-05T19:50:17","guid":{"rendered":"https:\/\/ecvp2025.uni-mainz.de\/?page_id=1276"},"modified":"2025-05-13T10:52:54","modified_gmt":"2025-05-13T10:52:54","slug":"conference","status":"publish","type":"page","link":"https:\/\/ecvp2025.uni-mainz.de\/conference","title":{"rendered":"Conference"},"content":{"rendered":"\n<div class=\"wp-block-group is-layout-flow wp-block-group-is-layout-flow\" style=\"padding-right:var(--wp--preset--spacing--20);padding-left:var(--wp--preset--spacing--20)\">\n<p>The ECVP is an annual international conference that aims to provide a forum for the presentation and discussion of new developments in the scientific study of visual perception. Empirical, theoretical, and applied perspectives from the disciplines of psychology, neuroscience, and cognitive science are all welcome and encouraged. Since 1978, ECVP has been one of the largest international conferences in the field, attracting researchers from all over the world. Please note that this year&#8217;s ECVP in Mainz will be an in-person meeting only.<\/p>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<div class=\"wp-block-group is-layout-flow wp-block-group-is-layout-flow\">\n<h3 class=\"wp-block-heading has-text-color has-link-color wp-elements-653ba9831a44213fd89737edc44798c8\" style=\"color:#325db9\">General Information<\/h3>\n\n\n\n<p><strong>ECVP 2025 starts on Sunday, August 24th with a series of hands-on workshops and more social warm-ups on science-related topics, and ends on Thursday, August 28th with the Farewell Party.<\/strong><\/p>\n\n\n\n<details class=\"wp-block-details is-style-default has-background is-layout-flow wp-block-details-is-layout-flow\" style=\"background-color:#888eba12\"><summary>More information &#8230;<\/summary>\n<p>On Sunday, <strong>August 24th<\/strong>, ECVP 2025 in Mainz will kick off with a series of workshops and warm-ups that will take place on the University Campus in the morning and early afternoon. On Sunday evening, the Perception Keynote Lecture will be held by Astrid Kappers<strong> <\/strong>(Eindhoven University of Technology) at the State Theater Mainz, followed by the Opening Reception.<br><br>From <strong>August 25th to August 28th<\/strong>, all symposia, talk sessions, poster sessions and keynote lectures will take place on the University Campus, just a short tram or bus ride from the main train station.<br><br>On Monday evening (<strong>August 25th<\/strong>), there will be an informal networking event for everybody (young at heart &lt;3) at the Old Mail Depot next to the main train station.<br><br>On Tuesday afternoon (<strong>August 26th<\/strong>), we will continue the Spotlight Lecture series, which showcases recent innovative and influential findings or methods in vision science. In 2025 the Spotlight in Vision Lecture will be given by Roland Fleming<strong> <\/strong>(Justus Liebig University of Giessen). In the evening, the popular Illusion Night will take place at the Culture Center KUZ, conveniently located in the heart of Mainz and within walking distance of numerous pubs and restaurants.<br><br>On Wednesday (<strong>August 27th<\/strong>), the traditional Rank Prize Lecture, this year given by William Warren (Brown University), will take place in the afternoon, and the Conference Dinner will be held in the evening at Heiliggeist, a venue with a historic ambience on the banks of the Rhine, a short walk from the main train station.<br><br>Finally, the conference will conclude on Thursday (<strong>August 28th<\/strong>) with talks and poster sessions running into the late afternoon. In the evening we will celebrate the end of the conference with the Farewell Party at the Old University Forum on the University Campus.<\/p>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<p>Free <strong>lunch <\/strong>will be provided during lunch breaks from August 25th to 28th. Free <strong>coffee<\/strong>, refreshments, and snacks will be available during all poster sessions. In addition, free drinks and <strong>finger food<\/strong> will be served during the Welcome Reception, the Illusion Night, and the Farewell Party.<\/p>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-custom-headings-color has-text-color has-link-color wp-elements-fa987a3272d411d4b83c910f86ec3bb8\">University Campus Mainz<\/h5>\n\n\n\n<p>The main conference venue is the campus of Johannes Gutenberg University in Mainz. It offers modern conference rooms and picturesque greenery for a walk between sessions.<\/p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-28f84493 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"577\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_1-1024x577.jpg\" alt=\"\" class=\"wp-image-3193\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_1-1024x577.jpg 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_1-300x169.jpg 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_1-768x433.jpg 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_1-1536x866.jpg 1536w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_1.jpg 1920w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/figure>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"579\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/ReWi_II-1024x579.jpg\" alt=\"\" class=\"wp-image-3175\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/ReWi_II-1024x579.jpg 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/ReWi_II-300x170.jpg 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/ReWi_II-768x434.jpg 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/ReWi_II-1536x869.jpg 1536w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/ReWi_II.jpg 1920w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/figure>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"576\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Botanischer_Garten-1024x576.jpg\" alt=\"\" class=\"wp-image-3184\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Botanischer_Garten-1024x576.jpg 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Botanischer_Garten-300x169.jpg 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Botanischer_Garten-768x432.jpg 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Botanischer_Garten-1536x864.jpg 1536w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Botanischer_Garten.jpg 1920w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/figure>\n<\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-28f84493 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"573\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Platz_vor_Mensa-1024x573.jpg\" alt=\"\" class=\"wp-image-3178\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Platz_vor_Mensa-1024x573.jpg 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Platz_vor_Mensa-300x168.jpg 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Platz_vor_Mensa-768x430.jpg 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Platz_vor_Mensa-1536x859.jpg 1536w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Platz_vor_Mensa.jpg 1920w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/figure>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"575\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_2-1024x575.jpg\" alt=\"\" class=\"wp-image-3196\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_2-1024x575.jpg 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_2-300x168.jpg 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_2-768x431.jpg 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_2-1536x862.jpg 1536w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/JGUEingang_2.jpg 1920w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/figure>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-large\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"574\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Alte_Mensa-1024x574.jpg\" alt=\"\" class=\"wp-image-3187\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Alte_Mensa-1024x574.jpg 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Alte_Mensa-300x168.jpg 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Alte_Mensa-768x430.jpg 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Alte_Mensa-1536x861.jpg 1536w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Alte_Mensa.jpg 1920w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><figcaption class=\"wp-element-caption\">All photos: Organizing Committee ECVP 2025<\/figcaption><\/figure>\n<\/div>\n<\/div>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<h5 class=\"wp-block-heading has-custom-headings-color has-text-color has-link-color wp-elements-bddab236774c13d2a5ee334dd25b6907\">Our logo<\/h5>\n\n\n\n<p>For ECVP 2025 in Mainz, our logo pays tribute to Johannes Gutenberg, the city&#8217;s most famous figure and the namesake of our university. Gutenberg is celebrated as the inventor of modern letterpress printing with movable types, a groundbreaking innovation that transformed communication. The design of our logo features movable types colored in different shades of the color wheel. This reflects not only the diversity within vision science, but also the ongoing evolution of how we communicate visual information. To enhance its visual appeal, gloss and shading effects have been added for depth.<br><br>In this way, the logo captures the essence of our conference theme and symbolizes the intersection of history and innovation in visual perception research.<\/p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1200\" height=\"237\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Logo_newLettersAddedDepth_Farbwechsel_1s.gif\" alt=\"Logo Colorchange\" class=\"wp-image-2449\" style=\"width:795px;height:auto\"\/><\/figure>\n<\/details>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-layout-flow wp-block-group-is-layout-flow\">\n<h3 class=\"wp-block-heading has-text-color has-link-color wp-elements-b60023f776cdb55229d125df0c6d4706\" style=\"color:#325db9\">Program<\/h3>\n\n\n\n<p><strong>Overview of our preliminary<\/strong>*<strong> conference schedule:<\/strong><\/p>\n\n\n\n<figure class=\"wp-block-image aligncenter size-large is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"723\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Schedule_ECVP_2025-1024x723.png\" alt=\"\" class=\"wp-image-3816\" style=\"width:832px;height:auto\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Schedule_ECVP_2025-1024x723.png 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Schedule_ECVP_2025-300x212.png 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Schedule_ECVP_2025-768x543.png 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Schedule_ECVP_2025-1536x1085.png 1536w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Schedule_ECVP_2025-2048x1447.png 2048w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/figure>\n\n\n\n<p>*Note: The days of the events are fixed, time slots may still change slightly.<\/p>\n\n\n\n<details class=\"wp-block-details has-background has-link-color wp-elements-daebe3ad12caec9543efa06dd6852561 is-layout-flow wp-block-details-is-layout-flow\" style=\"background-color:#888eba12\"><summary>More information and download of program &#8230;<\/summary>\n<p>The detailed online program will be available soon!<\/p>\n<\/details>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-layout-flow wp-block-group-is-layout-flow\">\n<h3 class=\"wp-block-heading has-text-color has-link-color wp-elements-318f3d2b5c6daf099e4e0c7f3e1876d1\" id=\"ConferenceSymposia\" style=\"color:#325db9\">Conference venues<\/h3>\n\n\n\n<p>With a click on the images below you will be redirected to a <a href=\"https:\/\/umap.openstreetmap.de\/de\/map\/ecvp-2025_74049?scaleControl=false&amp;miniMap=false&amp;scrollWheelZoom=true&amp;zoomControl=true&amp;editMode=disabled&amp;moreControl=true&amp;searchControl=null&amp;tilelayersControl=null&amp;embedControl=null&amp;datalayersControl=true&amp;onLoadPanel=caption&amp;captionBar=false&amp;captionMenus=true#15\/49.9955\/8.2540\" data-type=\"link\" data-id=\"https:\/\/umap.openstreetmap.de\/de\/map\/ecvp-2025_74049?scaleControl=false&amp;miniMap=false&amp;scrollWheelZoom=true&amp;zoomControl=true&amp;editMode=disabled&amp;moreControl=true&amp;searchControl=null&amp;tilelayersControl=null&amp;embedControl=null&amp;datalayersControl=true&amp;onLoadPanel=caption&amp;captionBar=false&amp;captionMenus=true#15\/49.9955\/8.2540\" target=\"_blank\" rel=\"noreferrer noopener\"><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\">map of all conference venues on OpenStreetMap<\/mark><\/a>.<\/p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-28f84493 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<h5 class=\"wp-block-heading has-custom-headings-color has-text-color has-link-color wp-elements-f95929ae8a27d60a3c9eb3d9533bd668\">Main conference venues on University Campus<\/h5>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Workshops &amp; warm-ups<\/strong> @ Law Building II <\/li>\n\n\n\n<li><strong>Spotlight in Vision Lecture<\/strong> &amp; <strong>Rank Prize Lecture<\/strong> @ Audimax <br>(Law Building I)<\/li>\n\n\n\n<li><strong>Talks <\/strong>&amp; <strong>symposia <\/strong>@ Old Refectory or Audimax (Law Building I)<\/li>\n\n\n\n<li><strong>Poster sessions<\/strong> &amp; <strong>coffee breaks<\/strong> @ Philosophicum<\/li>\n\n\n\n<li><strong>Lunch <\/strong>@ Mensa<\/li>\n\n\n\n<li><strong>Business Meetin<\/strong>g @ Audimax (Law Building I)<\/li>\n\n\n\n<li><strong>Farewell Party<\/strong> @ Old University Forum <\/li>\n<\/ul>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image alignright size-large is-resized\"><a href=\"https:\/\/umap.openstreetmap.de\/de\/map\/ecvp-2025_74049?scaleControl=false&amp;miniMap=false&amp;scrollWheelZoom=true&amp;zoomControl=true&amp;editMode=disabled&amp;moreControl=true&amp;searchControl=null&amp;tilelayersControl=null&amp;embedControl=null&amp;datalayersControl=true&amp;onLoadPanel=caption&amp;captionBar=false&amp;captionMenus=true#17\/49.99379\/8.23759\" target=\"_blank\" rel=\"noreferrer noopener\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"684\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_Campus_signs_100dpi-1024x684.png\" alt=\"\" class=\"wp-image-4305\" style=\"width:549px;height:auto\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_Campus_signs_100dpi-1024x684.png 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_Campus_signs_100dpi-300x200.png 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_Campus_signs_100dpi-768x513.png 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_Campus_signs_100dpi.png 1402w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/a><figcaption class=\"wp-element-caption\">Data by <a href=\"https:\/\/www.openstreetmap.org\/copyright\" target=\"_blank\" rel=\"noreferrer noopener\"><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\">OpenStreetMap<\/mark><\/a><\/figcaption><\/figure>\n<\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-28f84493 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<p class=\"has-custom-headings-color has-text-color has-link-color wp-elements-5e19188a6e95a180635fb2aee3ae6ab3\"><strong>External venues for some social events<\/strong><\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li><strong>Perception Lecture<\/strong> &amp; <strong>Welcome Reception<\/strong> (Aug 24th) @ State Theater <em>(Staatstheater)<\/em><\/li>\n\n\n\n<li><strong>PerceptioNite<\/strong> (Aug 25th) @ Old Mail Depot <em>(Altes Postlager)<\/em><\/li>\n\n\n\n<li><strong>Illusion &amp; Demo Night<\/strong> (Aug 26th) @ KUZ culture center<br><em>(Kulturzentrum KUZ)<\/em><\/li>\n\n\n\n<li><strong>Conference Dinner<\/strong> (Aug 27th) @ Heiliggeist<\/li>\n<\/ul>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image alignright size-large is-resized\"><a href=\"https:\/\/umap.openstreetmap.de\/de\/map\/ecvp-2025_74049?scaleControl=false&amp;miniMap=false&amp;scrollWheelZoom=true&amp;zoomControl=true&amp;editMode=disabled&amp;moreControl=true&amp;searchControl=null&amp;tilelayersControl=null&amp;embedControl=null&amp;datalayersControl=true&amp;onLoadPanel=caption&amp;captionBar=false&amp;captionMenus=true\" target=\"_blank\" rel=\"noreferrer noopener\"><img loading=\"lazy\" decoding=\"async\" width=\"1024\" height=\"469\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_city_signs_100dpi-1024x469.png\" alt=\"\" class=\"wp-image-3765\" style=\"width:550px;height:auto\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_city_signs_100dpi-1024x469.png 1024w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_city_signs_100dpi-300x137.png 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_city_signs_100dpi-768x352.png 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Map_city_signs_100dpi.png 1402w\" sizes=\"auto, (max-width: 1024px) 100vw, 1024px\" \/><\/a><figcaption class=\"wp-element-caption\">Data by <a href=\"https:\/\/www.openstreetmap.org\/copyright\" target=\"_blank\" rel=\"noreferrer noopener\"><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\">OpenStreetMap<\/mark><\/a><\/figcaption><\/figure>\n<\/div>\n<\/div>\n<\/div>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<div class=\"wp-block-group is-layout-flow wp-block-group-is-layout-flow\">\n<h3 class=\"wp-block-heading has-text-color has-link-color wp-elements-87c0d1111e97db9b09de9a7e75bd3dbb\" style=\"color:#325db9\">Keynotes<\/h3>\n\n\n\n<p><strong>We are thrilled to have secured three exceptional scientists as keynote speakers for ECVP 2025<\/strong>:<\/p>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-28f84493 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-group is-content-justification-left is-nowrap is-layout-flex wp-container-core-group-is-layout-fc9f69e7 wp-block-group-is-layout-flex\" style=\"min-height:80px\">\n<h4 class=\"wp-block-heading has-text-color has-link-color wp-elements-3909e95e3a7eda38e634a7b41f1f3df1\" style=\"color:#325db9\">Perception Lecture<\/h4>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-horizontal is-nowrap is-layout-flex wp-container-core-group-is-layout-edc3bc78 wp-block-group-is-layout-flex\" style=\"min-height:80px\">\n<p><strong>Astrid Kappers <\/strong><br>Eindhoven University <br>of Technology<\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<hr class=\"wp-block-separator has-alpha-channel-opacity is-style-dots\"\/>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:60px\">\n<p>Aug 24th @ State Theater | 18:00<\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:60px\">\n<p>Title: <strong>Exploring haptic perception<\/strong><\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<figure class=\"wp-block-image aligncenter size-full is-resized is-style-rounded is-style-rounded--1\"><img loading=\"lazy\" decoding=\"async\" width=\"2132\" height=\"2535\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Astrid.jpg\" alt=\"\" class=\"wp-image-4281\" style=\"width:auto;height:270px\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Astrid.jpg 2132w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Astrid-252x300.jpg 252w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Astrid-861x1024.jpg 861w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Astrid-768x913.jpg 768w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Astrid-1292x1536.jpg 1292w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Astrid-1722x2048.jpg 1722w\" sizes=\"auto, (max-width: 2132px) 100vw, 2132px\" \/><figcaption class=\"wp-element-caption\">Photo: Astrid Kappers<\/figcaption><\/figure>\n<\/div>\n\n\n\n<p>Laudatio: Jan Koenderink<\/p>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<p class=\"has-contrast-color has-text-color has-link-color wp-elements-718fcd76b10ff76cbd39372dd81bef52\">Sponsored by: <strong><a href=\"https:\/\/group.sagepub.com\/\" data-type=\"link\" data-id=\"https:\/\/group.sagepub.com\/\" target=\"_blank\" rel=\"noreferrer noopener\"><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\">SAGE &#8211; Perception \/ <em>i<\/em>-Perception<\/mark><\/a><\/strong><\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"992\" height=\"251\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Sage.png\" alt=\"\" class=\"wp-image-2740\" style=\"width:auto;height:80px\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Sage.png 992w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Sage-300x76.png 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/Sage-768x194.png 768w\" sizes=\"auto, (max-width: 992px) 100vw, 992px\" \/><\/figure>\n<\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:80px\">\n<h4 class=\"wp-block-heading has-text-color has-link-color wp-elements-294b4859edddf4089f12ecbc32c114fe\" style=\"color:#325db9\">Spotlight in Vision Lecture<\/h4>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:80px\">\n<p style=\"padding-top:0;padding-bottom:0\"><strong>Roland Fleming<\/strong><br>Justus Liebig University<br>Giessen<\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<hr class=\"wp-block-separator has-alpha-channel-opacity is-style-dots\"\/>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:60px\">\n<p class=\"has-contrast-color has-text-color has-link-color wp-elements-1f1c87482d240d1aaa0bf210a69754aa\">Aug 26th @ Audimax | 17:00<\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:60px\">\n<p>Title: <strong><strong>Visual Perception: past, present and future<\/strong><\/strong><\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<figure class=\"wp-block-image aligncenter size-full is-resized is-style-rounded is-style-rounded--2\"><img loading=\"lazy\" decoding=\"async\" width=\"810\" height=\"1080\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/810px-Roland_William_Fleming_2019.jpg\" alt=\"\" class=\"wp-image-3852\" style=\"width:auto;height:270px\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/810px-Roland_William_Fleming_2019.jpg 810w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/810px-Roland_William_Fleming_2019-225x300.jpg 225w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/810px-Roland_William_Fleming_2019-768x1024.jpg 768w\" sizes=\"auto, (max-width: 810px) 100vw, 810px\" \/><figcaption class=\"wp-element-caption\">Photo: Lina Klein<\/figcaption><\/figure>\n<\/div>\n\n\n\n<p>Laudatio: Karl Gegenfurtner<\/p>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<p class=\"has-contrast-color has-text-color has-link-color wp-elements-50d548dd583aae57f7c09d28b920007d\">Sponsored by: <a href=\"https:\/\/www.freunde.uni-mainz.de\/\"><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\"><strong>Freunde der Universit\u00e4t Mainz e.V.<\/strong><\/mark><\/a><\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-28f84493 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<figure class=\"wp-block-image size-full is-resized\"><img loading=\"lazy\" decoding=\"async\" width=\"864\" height=\"473\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2025\/04\/Freunde_JGU_gross_transparent.png\" alt=\"\" class=\"wp-image-5587\" style=\"width:auto;height:80px\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2025\/04\/Freunde_JGU_gross_transparent.png 864w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2025\/04\/Freunde_JGU_gross_transparent-300x164.png 300w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2025\/04\/Freunde_JGU_gross_transparent-768x420.png 768w\" sizes=\"auto, (max-width: 864px) 100vw, 864px\" \/><\/figure>\n<\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-columns is-layout-flex wp-container-core-columns-is-layout-28f84493 wp-block-columns-is-layout-flex\">\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\"><\/div>\n<\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-column is-layout-flow wp-block-column-is-layout-flow\">\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:80px\">\n<h4 class=\"wp-block-heading has-text-color has-link-color wp-elements-4be393b596a6313d64ff7430bcb7ceeb\" style=\"color:#325db9\">Rank Prize Lecture<\/h4>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:80px\">\n<p style=\"padding-top:0;padding-bottom:0\"><strong>William H.<\/strong> <strong>Warren<\/strong><br>Brown University<br>Providence<mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-dropdown-color\">.<\/mark><\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<hr class=\"wp-block-separator has-alpha-channel-opacity is-style-dots\"\/>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:60px\">\n<p class=\"has-contrast-color has-text-color has-link-color wp-elements-eb57c5483e4aba417f7fe9584326520b\">Aug 27th @ Audimax | 17:00<\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\" style=\"min-height:60px\">\n<p>Title: <strong>Perception, Action, and Information: Vison Outside-In<\/strong><\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<figure class=\"wp-block-image aligncenter size-full is-resized is-style-rounded is-style-rounded--3\"><img loading=\"lazy\" decoding=\"async\" width=\"480\" height=\"586\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/BillWarren_closeup.jpeg\" alt=\"\" class=\"wp-image-2707\" style=\"width:auto;height:270px\" srcset=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/BillWarren_closeup.jpeg 480w, https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/11\/BillWarren_closeup-246x300.jpeg 246w\" sizes=\"auto, (max-width: 480px) 100vw, 480px\" \/><figcaption class=\"wp-element-caption\">Photo: William Warren<\/figcaption><\/figure>\n<\/div>\n\n\n\n<p>Laudatio: James Todd<\/p>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<p class=\"has-contrast-color has-text-color has-link-color wp-elements-2071e1f79298480b91e2bc8c08e7a3c6\">Sponsored by: <strong><a href=\"https:\/\/www.rankprize.org\/\" data-type=\"link\" data-id=\"https:\/\/www.rankprize.org\/\" target=\"_blank\" rel=\"noreferrer noopener\"><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\">Rank Prize<\/mark><\/a><\/strong><br><\/p>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-nowrap is-layout-flex wp-container-core-group-is-layout-6c531013 wp-block-group-is-layout-flex\">\n<figure class=\"wp-block-image aligncenter size-full is-resized\"><img decoding=\"async\" src=\"https:\/\/ecvp2025.uni-mainz.de\/wp-content\/uploads\/2024\/12\/RankPrizeT.png\" alt=\"Rank Prize\" class=\"wp-image-878\" style=\"width:auto;height:80px\"\/><\/figure>\n<\/div>\n<\/div>\n<\/div>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n<\/div>\n\n\n\n<details class=\"wp-block-details has-background has-link-color wp-elements-82f517604e077de6db99a314a449ae1a\" style=\"background-color:#888eba12\"><summary><strong><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-headings-color is-layout-flow wp-block-details-is-layout-flow\">Perception Lecture<\/mark><\/strong> &#8211; <strong>Astrid Kappers<\/strong> &#8211; <strong><strong><em>Exploring haptic perception<\/em><\/strong><\/strong><br>Abstract:<\/summary>\n<p>The aim of my research is to gain an understanding of touch by means of a systematic and extensive exploration of haptic perception. In this talk, I will present an overview of some of the research we did in the last decades. Sometimes inspired by research on visual perception, our haptic research yielded many interesting and often surprising findings. To name just a few, we found that a simple task like making two bars haptically parallel yielded large but systematic deviations. We also observed strong aftereffects of haptic size or curvature perception: after touching an object for just a few seconds, perception of the size or curvature of the next object you touch will be influenced. When exploring raised-line drawings during a limited time (about 30 s), it was found that participants often were not able to recognise the object. However, when asked to draw what they had felt, they were able to recognise the object from their own drawing!&nbsp;<\/p>\n\n\n\n<p>One important take home message is that human haptic perception is often not veridical. This becomes relevant when designing man-machine interfaces. Recently, we became involved in the haptic perception research that is needed for designing haptic communication devices for persons with deafblindness. By means of vibration motors, haptic messages can be presented on the back or other body parts. When designing such devices, one should carefully consider how to present the haptic stimuli so that they are perceived as intended. I will end my talk with our latest experiments in this field.<\/p>\n<\/details>\n\n\n\n<details class=\"wp-block-details has-background has-link-color wp-elements-6da1f3508246606ff0b5dfbb7d532558\" style=\"background-color:#888eba12\"><summary><strong><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-headings-color is-layout-flow wp-block-details-is-layout-flow\">Spotlight in Vision Lecture<\/mark><\/strong> &#8211; <strong>Roland Fleming<\/strong> &#8211; <strong><strong><em>Visual Perception: past, present and future<\/em><\/strong><\/strong> <br>Abstract: <\/summary>\n<p>We&#8217;ve been studying visual perception for hundreds of years now. &nbsp;What have we learnt? &nbsp;Given what we know now, what are the grand open challenges for our field? &nbsp;And where does visual perception research fit in the ever-shifting landscape of AI, robotics and biomedical research? &nbsp;What does the future hold? &nbsp;While, I obviously can&#8217;t answer all these questions, through a combination of some first principles philosophising, and a little speculation, I&#8217;ll try to adumbrate some suggestions.<\/p>\n<\/details>\n\n\n\n<details class=\"wp-block-details has-background has-link-color wp-elements-d8cb9579329b9292062d8c287c49d617\" style=\"background-color:#888eba12\"><summary><strong><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-headings-color is-layout-flow wp-block-details-is-layout-flow\">Rank Prize Lecture<\/mark><\/strong> &#8211; <strong><strong><strong>William H.<\/strong><\/strong><\/strong> <strong><strong><strong>Warren<\/strong><\/strong><\/strong> &#8211; <strong><strong><em><strong><em>Perception, Action, and Information: Vison Outside-In<\/em><\/strong><\/em><\/strong><\/strong><br>Abstract: <\/summary>\n<p>It\u2019s a perplexing time in the study of visual perception. On the one hand, there is a resurgence of models that attribute <em>a priori<\/em> structure to the visual system, such as priors, generative world models, and physics engines. On the other hand, there is the astonishing <em>a posteriori<\/em> success of deep neural networks trained only on natural images and image sequences. Although their performance offers an existence proof of the sufficiency of information in natural images for certain visual tasks, the black box of deep learning does not easily reveal what that information is or how it\u2019s extracted by the visual system.&nbsp;<\/p>\n\n\n\n<p>A science of perception still depends on understanding the visual information that is available in natural environments and is used to guide natural behavior. I propose that we take seriously James Gibson\u2019s information hypothesis: <em>For every perceivable property of the environment, however subtle, there must be a variable of information, however complex, that uniquely specifies it<\/em>. The project is to identify the information that the visual system uses to perceive and act within the constraints of our ecological niche. You might think of this as vision from outside-in, rather than inside-out.&nbsp;<\/p>\n\n\n\n<p>Two decades ago I decided to work out a test case to see whether an information-based account of a natural behavior could be sustained. In this talk I will offer a status report on our effort to build a <em>pedestrian model<\/em> \u2013 a model of visually controlled locomotion \u2013 that scales up from individual behaviors like steering and obstacle avoidance, to pedestrian interactions like following and collision avoidance, to the collective behavior of human crowds. Surprisingly, linear combinations of these nonlinear components can account for the emergence of more complex behavior, such as self-organized \u2018flocking\u2019, crowd bifurcations, and stripe formation in crossing flows.&nbsp;<\/p>\n<\/details>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n\n\n\n<div class=\"wp-block-group is-layout-flow wp-block-group-is-layout-flow\">\n<h3 class=\"wp-block-heading has-text-color has-link-color wp-elements-962e292abf99aebd78e6a9994fd3429c\" id=\"ConferenceSymposia\" style=\"color:#325db9\">Symposia<\/h3>\n\n\n\n<p>Our symposia offer a diverse selection of research topics, featuring speakers from various career stages, research backgrounds, and global locations to encourage lively discussion and exchange of ideas. A symposium should provide a diverse overview of a research area of interest to the ECVP audience. Organizers should strive to include speakers from different career stages, research groups, and geographic locations who represent a broad range of views and ideas under the overarching theme of the symposium. Presentations should be related to each other and stimulate insightful discussion. The symposium schedule will be announced near the conference.<\/p>\n\n\n\n<p>Please find more information about our submission guidelines on the<mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\"> <\/mark><a href=\"https:\/\/ecvp2025.uni-mainz.de\/submissions\/\" data-type=\"link\" data-id=\"https:\/\/ecvp2025.uni-mainz.de\/submissions\/\"><mark style=\"background-color:rgba(0, 0, 0, 0)\" class=\"has-inline-color has-custom-links-color\">Submissions<\/mark><\/a> page.<\/p>\n\n\n\n<p><strong>Symposium submissions are complete. For a detailed list of all symposia, including the preliminary schedule of speakers, please expand the item below.<\/strong><\/p>\n\n\n\n<details class=\"wp-block-details has-background is-layout-flow wp-block-details-is-layout-flow\" style=\"background-color:#888eba12\"><summary>LIST OF SYMPOSIA:<\/summary>\n<h5 class=\"wp-block-heading\">1) Examining vision and visual dysfunction with advanced neuroimaging<\/h5>\n\n\n\n<p>Chairs: Antony Morland &amp; Shahin Nasr<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Netta Levin &amp; Ruth Abulafia: <em>Cortical visual field representation and data integration following optic neuritis<\/em><\/li>\n\n\n\n<li>Michael B. Hoffmann: <em><em>Consequences of congenital malformations of the optic chiasm on the visual brain<\/em><\/em><\/li>\n\n\n\n<li>Shahin Nasr: <em>Using high-resolution fMRI to investigate the effects of amblyopia on the mesoscale functional organization of the human visual cortex<\/em><\/li>\n\n\n\n<li>Hinke N. Halbertsma, Shereif Haykal, Hanna Willis, Holly Bridge, &amp; Frans W. Cornelissen: <em>Fixel-based analysis of diffusion-weighted imaging data to assess neurodegeneration in homonymous hemianopia<\/em><\/li>\n\n\n\n<li>Holly Bridge, Hanna Willis, &amp; I Betina Ip: <em>Using magnetic resonance spectroscopy to investigate the role of neurochemistry in human visual system function and dysfunction<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">2) <strong>Dealing with the visual consequences of eye and head movements: Recent findings and implications<\/strong><\/h5>\n\n\n\n<p>Chairs: David Souto &amp; Alexander Sch\u00fctz<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Antonella Pom\u00e8 &amp; Eckart Zimmermann: <em>Sensory Census: how efference copies from eye movements determine the number of objects we see in dynamic environments<\/em><\/li>\n\n\n\n<li>Ziad Hafed: <em>Dark contrasts are immune to saccadic suppression in the primary visual cortex<\/em><\/li>\n\n\n\n<li>David Souto, Omar Bachtoula, Mel Ellul Miraval, &amp; Ignacio Serrano-Pedraza: <em>Sensory and motor suppression of optokinesis during smooth pursuit eye movements<\/em><\/li>\n\n\n\n<li>Rozana Ovsepian, David Souto, &amp; Alexander C. Sch\u00fctz: <em>Robust generalization of tuning to self-induced sensation<\/em><\/li>\n\n\n\n<li>Paul MacNeilage: <em>The role of oculomotor signals in stationarity perception<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">3) Perceiving visual actions: eye movement awareness and sensorimotor control in active vision<\/h5>\n\n\n\n<p>Chairs: Jan-Nikolas Klanke &amp; Wiebke N\u00f6renberg<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Amit Rawal &amp; Rosanne L. Rademaker: <em>People are sensitive to their uniquely patterned retinal input<\/em><\/li>\n\n\n\n<li>Jan-Nikolas Klanke, Sven Ohl, Almila Naz Esen, &amp; Martin Rolfs: <em>Eyes on target, awareness off course: Limited control and detection of catch-up saccades during pursuit eye movements<\/em><\/li>\n\n\n\n<li>Alexander Goettker, Jolande Fooken, Shannon Locke, Karl Gegenfurtner, &amp; Pascal Mamassian: <em>Limited metacognitive awareness of eye movement accuracy: Insights from saccade and tracking tasks<\/em><\/li>\n\n\n\n<li>Anne Helen Hoffmann, Ilana Nisky, &amp; Fr\u00e9d\u00e9ric Crevecoeur: <em>Task and control-demands influence the use of visual feedback during arm movement control<\/em><\/li>\n\n\n\n<li>Wiebke N\u00f6renberg, Pascal Mamassian, &amp; Martin Rolfs: <em>In the eye of the timer: Rising up to the challenges of subjective saccade timing<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">4) From understanding low-level visual processes to tackling key societal challenges &#8211; the changing role of vision research<\/h5>\n\n\n\n<p>Chairs: Olivier Penacchio &amp; Ute Leonards<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Branka Spehar: <em>Neural, perceptual, and affective responses to variations in natural scene statistics<\/em><\/li>\n\n\n\n<li>Claudia Menzel: <em>Effects of lower- and higher-level processed properties for the restorativeness of nature and urban images<\/em><\/li>\n\n\n\n<li>Tade\u00e1\u0161 Dvo\u0159\u00e1k, Kate\u0159ina Ingrov\u00e1, Radek Mare\u010dek, Filip Zl\u00e1mal, &amp; Julie Dobrovoln\u00e1: <em>Biomarkers of exposure to nature and urban environments<\/em><\/li>\n\n\n\n<li>Jan Mikuni: <em>Visual aesthetic considerations on urban landscapes<\/em><\/li>\n\n\n\n<li>Jay Davies, Ute Leonards, &amp; Jasmina Stevanov: <em>Challenging the nature versus urban dichotomy: Aligning research classifications with human visual perception.<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">5) Self-motion without actual motion: trends in visual vection research from basic neuro-cognitive processing to clinical applications<\/h5>\n\n\n\n<p>Chairs: Stefan Berti &amp; Behrang Keshavarz<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Robert Allison, Laurie Wilcox, Hongyi Guo, &amp; Xue Teng: <em>Object motion while experiencing vection<\/em><\/li>\n\n\n\n<li>Pawe\u0142 Str\u00f3\u017cak, Tomasz Jankowski, Marcin Wojtasi\u0144ski, &amp; Pawe\u0142 Augustynowicz: <em>Individual-difference factors modulating the experience of vection. The role of field dependence, anomalous perceptual experiences, and tolerance of ambiguity<\/em><\/li>\n\n\n\n<li>Stefan Berti, Brandy Murovec, Susan Yahya, Julia Spaniol, &amp; Behrang Keshavarz: <em>Early cortical processing of vection during coherent vs. non-coherent motion stimuli in younger and older adults: An event-related potential (ERP) study<\/em><\/li>\n\n\n\n<li>Michaela McAssey, Lena Fabritius, Geraldine Tauber, Valerie Kirsch, Thomas Brandt, &amp; Marianne Dieterich: <em>Combining EEG and vection to investigate visual-vestibular interactions in healthy and clinical populations<\/em><\/li>\n\n\n\n<li>Grace Gabriel, Meaghan Adams, Behrang Keshavarz, Lauren Sergio, &amp; Jennifer Campos: <em>Vection in individuals with and without concussion: Associations with postural responses and visual dependence<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">6) Individual differences in perceptual and sensorimotor processing: A look into real-world expertise<\/h5>\n\n\n\n<p>Chairs: Jolande Fooken &amp; Alexander Goettker<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Lynn Schmittwilken, Anna. L. Haverkamp, &amp; Marianne Maertens: <em>Quantifying human edge sensitivity in real-world tasks<\/em><\/li>\n\n\n\n<li>Dominik Straub, Lukas Maninger, &amp; Constantin A. Rothkopf: <em>Estimating individual differences in perceptual, cognitive, and motor processes from behavior in tracking tasks<\/em><\/li>\n\n\n\n<li>Ashima Keshava &amp; Peter K\u00f6nig: <em>Adaptive actions and frugal memory: How gaze supports natural behavior<\/em><\/li>\n\n\n\n<li>Jolande Fooken, Renato Moraes, J. Randall Flanagan, &amp; Constantin A. Rothkopf: <em>Old hands take more time: Healthy ageing is associated with subtle changes in visuomotor control when grasping and dropping objects<\/em><\/li>\n\n\n\n<li>Roy Hessels, Toshiki Iwabuchi, Diederick Niehorster, Ren Funawatari, Jeroen Benjamins, Sayaka Kawakami, Marcus Nystr\u00f6m, Momoka Suda, Ignace Hooge, Motofumi Sumiya, Julie Heijnen, Martin Teunisse, &amp; Atsushi Senju: <em>Gaze behavior in face-to-face interaction: A cross-cultural investigation between Japan and the Netherlands<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">7) Is one test sufficient?<\/h5>\n\n\n\n<p>Chair: Michael Herzog<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Michael H. Herzog, Melissa Faggella, &amp; Simona Garobbio: <em>About noise and inter-participant variability<\/em><\/li>\n\n\n\n<li>Anna-Lena Schubert: <em>Individual differences in the speed of visual processing are stable across time but only moderately consistent across tasks<\/em><\/li>\n\n\n\n<li>Amelia Hunt, Anna Nowakowska, &amp; Alasdair Clarke: <em>Individual differences in visual search and the fallacy of misplaced concreteness<\/em><\/li>\n\n\n\n<li>Jenny Bosten, Patrick Goodbourn, Gary Bargary, Adam Lawrance-Owen, Ruth Hogg, Roeland Verhallen, &amp; John Mollon: <em>Correlated and uncorrelated individual differences in performance on a diverse set of psychophysical and oculomotor tasks<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">8) Temporal dependence on visual perception: Quo vadis?<\/h5>\n\n\n\n<p>Chairs: Mauro Manassi &amp; David Pascucci<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Mauro Manassi, Fiammetta Marini, Linda Jeffery, &amp; Clare Sutherland: <em>Time, space and feature similarity determine repulsive and attractive serial biases in trustworthiness impressions<\/em><\/li>\n\n\n\n<li>Emma Stewart &amp; Alex Goettker: <em>Cognitive and retinal components of serial dependence in oculomotor control<\/em><\/li>\n\n\n\n<li>Merav Ahissar: <em>Can we manipulate context effects by task instructions?<\/em><\/li>\n\n\n\n<li>Koulla Mikellidou:<em> Serial dependence in continuous and interrupted motion perception in an immersive virtual environment<\/em><\/li>\n\n\n\n<li>David Pascucci: <em>Context effects in perceptual decision-making: for better or worse?<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">9) Out of sight, but not out of mind: How the human brain represents images that are not directly seen<\/h5>\n\n\n\n<p>Chair: Rosanne Rademaker<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Rosanne L. Rademaker: <em>The question of representational formats in working memory<\/em> (symposium introduction)<\/li>\n\n\n\n<li>Maria V. Servetnik &amp; Rosanne L. Rademaker: <em>Early visual cortex is recruited to act as a comparison circuit between mental representations and visual inputs<\/em><\/li>\n\n\n\n<li>Joana Pereira Seabra, Andreea-Maria Gui, Carsten Allefeld, Vivien Chopurian, Alessandra S. Souza, &amp; Thomas B. Christophel: <em>Multiple formats of visuo-spatial working memory<\/em><\/li>\n\n\n\n<li>Bradley Postle: <em>Representational formats to encode context and priority in visual working memory<\/em><\/li>\n\n\n\n<li>Clayton Curtis, Ziyi Duan, &amp; Nathan Tardiff: <em>Behavioral demands shape the format of visual working memory<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">10) Perception of non-rigid motions<\/h5>\n\n\n\n<p>Chairs: Krischan Koerfer &amp; Markus Lappe<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Takahiro Kawabe: <em>Nonrigid motion perception underlying material and animacy impressions<\/em><\/li>\n\n\n\n<li>Merve Erdogan, Wenyan Bi, Ilker Yildirim, &amp; Brian Scholl: <em>Rich non-rigid percepts, beyond biology: Perceiving point-light cloths waving in the wind<\/em><\/li>\n\n\n\n<li>Jiayi Pang &amp; William Warren: <em>Spatiotemporal integration of motion in human crowds<\/em><\/li>\n\n\n\n<li>Krischan Koerfer &amp; Markus Lappe: <em>Nonrigid motion perception and eye movements<\/em><\/li>\n\n\n\n<li>Roland Fleming: <em>Motion cues and the perception of materials<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">11) Specificity and generalization of learning<\/h5>\n\n\n\n<p>Chair: Giorgio Manenti<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Caspar M. Schwiedrzik: <em>Stimulus variability during training breaks the \u201ccurse of specificity\u201d in visual perceptual learning<\/em><\/li>\n\n\n\n<li>Rosanne Rademaker: <em>Effects of statistical regularities on representation and behavior<\/em><\/li>\n\n\n\n<li>Can Demircan: <em>Evaluating alignment between humans and neural network representations in image-based learning tasks<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">12) The social symphony of gaze: New perspectives on eye contact behaviour<\/h5>\n\n\n\n<p>Chairs: Naiqi G. Xiao &amp; Nikolaus Troje<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Shreshth Saxena &amp; Lauren Fink: <em>An automated method for multi-person mobile eye-tracking in natural contexts involving shared gaze goals<\/em><\/li>\n\n\n\n<li>Nikolaus F. Troje, Kristen Lott, Zahra Hosseini, &amp; Nicholas Logan: <em>Social gaze in video conferencing<\/em><\/li>\n\n\n\n<li>Florence Mayrand &amp; Jelena Ristic: <em>Who Looks, When, and Why? Linking gaze behaviors in natural interactions with group and individual social function<\/em><\/li>\n\n\n\n<li>Prasetia Utama Putra &amp; Fumihiro Kano: <em>Decoding joint action success through eye movements: A data-driven approach<\/em><\/li>\n\n\n\n<li>Sara Ripley, Wei Fang, Gabriel (Naiqi) Xiao, &amp; Laurel Trainor: <em>The early emerged sensitive to social signals and gaze interactions between mother-infant interactions<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">13) Understanding gaze<\/h5>\n\n\n\n<p>Chair: Anke Huckauf<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Gernot Horstmann &amp; Linda Linke: <em>Active and passive perception of direct gaze<\/em><\/li>\n\n\n\n<li>Mehtap Cakir &amp; Anke Huckauf: <em>Recognition of mental processes of others based on gaze characteristics<\/em><\/li>\n\n\n\n<li>Enkelejda Kasneci: <em>Towards imperceptible gaze guidance in extended reality<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">14) Active vision in embodied interaction<\/h5>\n\n\n\n<p>Chair: Vasiliki Kondyli<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>\u00c1rni Kristj\u00e1nsson, Jennifer Magerl-Fuller, &amp; \u00c1rni Gunnar \u00c1sgeirsson: <em>Probabilistic attention templates guide visual selection<\/em><\/li>\n\n\n\n<li>Marcin Leszczy\u0144ski: <em>Neurophysiology of active vision<\/em><\/li>\n\n\n\n<li>Vasiliki Kondyli: <em>Adaptive gaze behavior and active predictions. Multimodal behavioural studies in dynamic environments<\/em><\/li>\n\n\n\n<li>Mehul Bhatt: <em>Towards responsible AI foundations for neurocognitive analytics of active vision<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">15) Visual representations of bodies: Neural and computational mechanisms of action and social perception<\/h5>\n\n\n\n<p>Chairs: Martin A. Giese &amp; Beatrice de Gelder<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Rufin Vogels: <em>Representations of static and dynamic bodies in macaque visual cortex<\/em><\/li>\n\n\n\n<li>Alexander Lappe, Anna Bogn\u00e1r, Rufin Vogels, &amp; Martin A. Giese: <em>Shared-feature visualization by parallel backpropagation for body-selective neurons in the STS<\/em><\/li>\n\n\n\n<li>Marius Zimmermann &amp; Angelika Lingnau: <em>Time course of neural midlevel representations underlying action recognition<\/em><\/li>\n\n\n\n<li>Beatrice de Gelder: <em>How body perception contributes to social interaction<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">16) Rethinking the role of brain rhythms in vision: Predictive dynamics, temporal sampling, and individual differences<\/h5>\n\n\n\n<p>Chairs: David Melcher, Daniel Kaiser, &amp; Gianluca Marsicano<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Lu-Chun Yeh, Max Bardelang, &amp; Daniel Kaiser:<em> Alpha rhythms track occluded motion in natural scene perception<\/em><\/li>\n\n\n\n<li>Michele Deodato &amp; David Melcher: <em>The relevance of alpha phase for visual processing<\/em><\/li>\n\n\n\n<li>Ma\u00eblan Q. Men\u00e9trey, Michael H. Herzog, &amp; David Pascucci: <em>Beyond the alpha cycle: how alpha activity shapes stable traits and transient dynamics in visual temporal integration<\/em><\/li>\n\n\n\n<li>Giuseppe Di Dona, Alessia Santoni, Sara Stottmeier, Klara Hemmerich, &amp; Luca Ronconi: <em>Oscillatory dynamics underlying predictive coding in motion perception<\/em><\/li>\n\n\n\n<li>Gianluca Marsicano &amp; David Melcher: <em>Atypical weighting of sensory evidence and perceptual priors in causality perception along the ASD-SCZ continuum<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">17) Where and when? Modeling motion prediction<\/h5>\n\n\n\n<p>Chair: Daniel Oberfeld-Twistel<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Joan Lopez-Moliner, David Aguilar-Lleyda, &amp; Cristina de la Malla:<em> How scene variability affects time-to-contact estimation and its use in decision-making<\/em><\/li>\n\n\n\n<li>Daniel Oberfeld-Twistel &amp; Tim Niewalda: <em>Simple Bayesian observer models explain important characteristics of visual time-to-collision estimation in a street-crossing scenario<\/em><\/li>\n\n\n\n<li>Borja Aguado &amp; Loes C. J. van Dam: <em>Explaining the angle-of-approach and curveball effects in interception with an LQG model that combines trajectory prediction and implicit goal costs<\/em><\/li>\n\n\n\n<li>Oh-Sang Kwon, Hyun-Jun Jeon, &amp; Duje Tadin: <em>Discrete percepts of continuously moving objects<\/em><\/li>\n\n\n\n<li>Constantin Rothkopf, Dominik Straub, &amp; Tobias Niehues: <em>Intercepting moving targets: from optimal control to TTC<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">18) <strong>Using interocular suppression in consciousness research: Current state and future directions<\/strong><\/h5>\n\n\n\n<p>Chairs: Renzo Lanfranco, Tommaso Ciorli, &amp; Timo Stein<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Surya Gayet:<em> Perceptual precedence for expected and dreaded visual events \u2013 evidence from \u2018bias-free\u2019 breaking continuous flash suppression<\/em><\/li>\n\n\n\n<li>Tommaso Ciorli: <em>Disentangling conscious and unconscious processing in interocular suppression: the rev-bCFS paradigm<\/em><\/li>\n\n\n\n<li>Cordula Hunt, Nikola Peise, Florian Kobylka, &amp; Guenter Meinhardt: <em>Temporal summation reveals different levels of feature integration under interocular suppression<\/em><\/li>\n\n\n\n<li>Renzo Lanfranco: <em>Beyond interocular suppression: Unmasked sub-millisecond presentations reveal visual processing priorities in perception and awareness<\/em><\/li>\n\n\n\n<li>Timo Stein, Theyn Kan, Rosa Moesker, &amp; Micha Heilbron: <em>Unpredictability accelerates conscious access during natural scene perception: Evidence from breaking CFS<\/em><\/li>\n<\/ul>\n\n\n\n<p><strong>19) Sensing the future: Multisensory, aesthetics and sustainable insights in material perception<\/strong><\/p>\n\n\n\n<p>Chair: Marella Campagna<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Claus-Christian Carbon: <em>Beyond vision: The multisensory nature of aesthetics<\/em><\/li>\n\n\n\n<li>Marella Campagna, Alexander Pastukhov, &amp; Claus-Christian Carbon: <em>Multisensory aesthetic perception: A quantitative-qualitative study on visuo-tactile interactions with material textures<\/em><\/li>\n\n\n\n<li>Lotta Straube, Alexander Pastukhov, Lisa Alexandra Gromer,\u00a0Anna Heuschkel, &amp; Claus-Christian Carbon: <em>Sustainable product and material perception: A multisensory exploration of denim jeans<\/em><\/li>\n\n\n\n<li>Pik Ki Ho &amp; Mohamed Al Musleh: <em>Recording the aesthetic responses and designer-visitor interactions during a design exhibition \u2013 an eye movement and motion tracking study<\/em><\/li>\n<\/ul>\n\n\n\n<p><strong>20) The perception of the visual world &#8211; 75 years later<\/strong><\/p>\n\n\n\n<p>Chair: Klaus Landwehr<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>James Todd: <em>Optical gradients as sources of visual information<\/em><\/li>\n\n\n\n<li>Brian Rogers: <em>Has Gibson&#8217;s (1950) characterisation of &#8220;The Stimulus Variables for Visual Depth and Distance&#8221; stood the test of time?<\/em><\/li>\n\n\n\n<li>Klaus Landwehr, Heiko Hecht, &amp; Christoph von Castell: <em>Texture gradients are live and well<\/em><\/li>\n\n\n\n<li>Jan J. Koenderink: <em>The focus of expansion<\/em><\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">21) Visual illusions and related phenomena as tools for understanding perception: A symposium in honor of Lothar Spillmann<\/h5>\n\n\n\n<p>Chairs: Allison Sekuler, Jeremy Wolfe, &amp; Mark Greenlee<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>Birgitta Dresp-Langley: <em>Bilateral symmetry and figure-ground segregation<\/em><\/li>\n\n\n\n<li>Chia-huei Tseng &amp; Hiu Mei Chow: <em>Study Visual Illusions Outside the Research Lab: Hong Kong Peak Tram Illusion and Its Implications to our Verticatility Perception<\/em><\/li>\n\n\n\n<li>Fr\u00e9d\u00e9ric Devinck: <em>Color filling-in: an example with the Watercolor effect<\/em><\/li>\n\n\n\n<li>Baingio Pinna: <em>The hierarchical facial feature integration theory: A new paradigm in face perception<\/em><\/li>\n\n\n\n<li>John S. Werner: <em>Long-range Interactions &#8211; A Personal History of a Long-term Collaboration between Laboratories<\/em><\/li>\n<\/ul>\n<\/details>\n\n\n\n<div style=\"height:20px\" aria-hidden=\"true\" class=\"wp-block-spacer\"><\/div>\n<\/div>\n\n\n\n<div class=\"wp-block-group is-layout-flow wp-block-group-is-layout-flow\">\n<h3 class=\"wp-block-heading has-text-color has-link-color wp-elements-b55b1212e6a91f6317e525a4c08626f1\" style=\"color:#325db9\">Workshops and Warm-ups<\/h3>\n\n\n\n<p>On Sunday, August 24th, a series of hands-on science skill workshops and warm-ups on science-related issues will be held on the University Campus in the Law Building II. We will have a total of 10 sessions, with sessions running concurrently in the morning from 10:00-12:30 and in the afternoon from 13:30-16:00. Since sessions are running concurrently, you may register for only one session in the morning and one in the afternoon. All tutorials are listed below with a short abstract and free registration for the workshops is now open via our booking platform Converia.<br><strong>All workshop slots are now filled with interesting sessions &#8211; many thanks to all our workshop organizers!<\/strong><\/p>\n\n\n\n<details class=\"wp-block-details is-style-default has-custom-dropdown-background-color has-background is-layout-flow wp-block-details-is-layout-flow\"><summary>LIST OF WORKSHOPS:<\/summary>\n<h4 class=\"wp-block-heading\">Workshops in the morning (from 10:00-12:30)<\/h4>\n\n\n\n<h5 class=\"wp-block-heading\">1) Unity game engine for beginners: Creating virtual reality applications and serious games<\/h5>\n\n\n\n<p>Organizer: Alessandro Forgiarini (University of Udine)<\/p>\n\n\n\n<p>This beginner-friendly workshop introduces the Unity Game Engine as a powerful tool for creating Virtual Reality (VR) applications and Serious Games. These technologies are widely used in the literature for education, therapy, and training.<br>The workshop is designed for individuals new to game development and offers a step-by-step approach to building an engaging and immersive project. Participants will learn the fundamentals of Unity, including navigating the interface, creating 3D scenes, managing assets, and using basic C# scripting to add interactivity. The workshop will emphasize how VR technology influences can enhance user engagement and allow to design meaningful and intuitive experiences.&nbsp;<br>After a brief introduction to VR hardware, attendees will create a simple VR Serious Game through guided activities. No prior experience with programming or Unity is required, making this session accessible to everyone.<\/p>\n\n\n\n<p><strong>Please note:<\/strong> <em>This workshop consists of two sessions that build on one another (Part I &amp; Part II in the afternoon)!<\/em><\/p>\n\n\n\n<h5 class=\"wp-block-heading\">2) Open and FAIR stimulus creation with stimupy<\/h5>\n\n\n\n<p>Organizer: Lynn Schmittwilken &amp; Joris Vincent (Technische Universit\u00e4t Berlin)<\/p>\n\n\n\n<p>Stimuli are at the heart of vision science, yet are not always openly accessible. Stimupy (Schmittwilken, Maertens, &amp; Vincent, 2023) tackles this problem, and makes stimulus creation findable, accessible, interoperable, and reusable (FAIR). Stimupy is an open-source Python package for creating two-dimensional stimuli to test and\/or control aspects of early\/mid-level vision, including shapes, gratings, visual illusions, and noises. In this tutorial, we introduce you to FAIR in the context of stimulus creation, and show you how you can use stimupy for a wide range of research purposes, such as experimentation, modeling, replication, and the exploration of stimulus parameter spaces.<\/p>\n\n\n\n<p>Attendants should bring a laptop to fully participate in the workshop. They will get the most out of it if they already have&nbsp;<a href=\"https:\/\/www.python.org\/\" target=\"_blank\" rel=\"noreferrer noopener\">Python<\/a>&nbsp;installed (any recent version 3.9+ suffices), as well as stimupy and its dependencies &#8211; the installation instructions from&nbsp;<a href=\"https:\/\/stimupy.readthedocs.io\/en\/latest\/getting_started\/installation.html\" target=\"_blank\" rel=\"noreferrer noopener\">stimupy.readthedocs.io<\/a>&nbsp;should cover the usual cases.<\/p>\n\n\n\n<h5 class=\"wp-block-heading\">3) A practical introduction to evidence accumulation models in visual perception research<\/h5>\n\n\n\n<p>Organizer: Margherita Calderan &amp; Carolina Maria Oletto (University of Padua)<\/p>\n\n\n\n<p>This tutorial is designed to provide the foundations for using evidence accumulation models on experiments involving perceptual judgements. In visual perception, researchers frequently rely just on participants&#8217; choices. This approach is valuable, but it overlooks the temporal information embedded within response times, which can reflect underlying perceptual processes (Corbett &amp; Smith, 2020; Hellman et al., 2024; Rushton et al., 2024). Evidence accumulation models offer a more comprehensive approach by jointly considering participants&#8217; choices and response times (Ratcliff, &amp; McKoon, 2008). Moreover, these models can also handle multiple-choice scenarios (two or more; Heathcote, &amp; Matzke, 2022), allowing the analysis of responses in a broader range of visual perception tasks. This tutorial will introduce the rationale for using evidence accumulation models in visual perception research. Real research data will be presented and analysed through bayesian multilevel models. A basic knowledge of Bayesian statistics is useful for a deeper understanding of the tutorial, but is not mandatory. Please bring your laptop with R and Rstudio already installed.<\/p>\n\n\n\n<h5 class=\"wp-block-heading\">4) R you serious? Teaching R programming to psych students<\/h5>\n\n\n\n<p>Organizer: Meike Steinhilber (Johannes Gutenberg University Mainz)<\/p>\n\n\n\n<p>Teaching R in psychology courses often presents unique challenges: students may lack prior programming experience, time is limited, and motivation to learn coding can be low. Yet, mastering R is essential for conducting statistical analyses effectively. How can we best teach R in a way that is engaging, accessible, and pedagogically sound?<\/p>\n\n\n\n<p>This workshop will explore both technical and instructional strategies for teaching R. We will introduce&nbsp;<strong>learnr<\/strong>, an interactive learning package, and showcase&nbsp;<strong>OtteR<\/strong>, a tool designed to support R education. Beyond these resources, we will discuss best practices for structuring R courses, balancing statistical concepts with programming fundamentals, and collaboratively developing ideas to enhance long-term student engagement.<\/p>\n\n\n\n<p>Whether you are an experienced instructor looking to refine your teaching approach or new to teaching R, this session welcomes anyone interested in improving R education in psychology. We also encourage seasoned educators to share their experiences, challenges, and successful strategies, fostering a collaborative exchange of insights and best practices.<\/p>\n\n\n\n<p>To get the most out of this session, please bring:<\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>A tablet or laptop to explore OtteR in real time.<\/li>\n\n\n\n<li>Your current R curriculum to serve as a foundation for discussion.<\/li>\n\n\n\n<li>A list of questions, challenges, or key topics related to teaching R that you\u2019d like to explore and troubleshoot together.<\/li>\n<\/ul>\n\n\n\n<h5 class=\"wp-block-heading\">5) You should write an R package. It is easier than you think, and I\u2019ll show you how.<\/h5>\n\n\n\n<p>Organizer: Alexander (Sasha) Pastukhov (University of Bamberg)<\/p>\n\n\n\n<p>In our research, we generate innovative and useful analysis methods. Yet, reusing these methods across projects or sharing them as plain R scripts or notebooks can often be challenging or awkward. Packaging your code as an open-source library available at Github and CRAN not only facilitates reuse in new projects but also simplifies collaboration by ensuring your tools are accessible to the broader scientific community.The task may initially seem daunting, and you might feel that package-writing is best left to trained programmers. However, R and RStudio offer an excellent suite of tools that make transforming your code into a well-functioning, well-documented, and easy-to-install package surprisingly straightforward (or, at least, easier than you might think). Moreover, converting your code into a package \u2013 especially one that meets CRAN standards \u2013 encourages you to address aspects that might not be part of your usual workflow. This process will push you to write clear documentation complete with practical examples, prepare illustrative datasets, and test your code not only to confirm that it works but also to ensure it fails gracefully when expected.This workshop aims to provide a comprehensive overview of the entire package creation process. We will cover everything from creating an empty project and adding functions or classes to writing comprehensive documentation and practical examples (since poorly structured documentation is often the primary barrier to using your methods), as well as including and documenting example data, creating vignettes, and testing your package. Additionally, you will learn how to make your package installable from GitHub, publish documentation via GitPages, prepare it for CRAN submission, and ensure that your package is properly cited. I will introduce you to the tools R and RStudio provide for each step and how to automate tasks to streamline package creation. Together, we will build a simple yet feature-complete library that you can use as a stepping stone for your future packages.<\/p>\n\n\n\n<p><strong>Prerequisite Knowledge:<\/strong>&nbsp;Familiarity with R and, optionally, Git<\/p>\n\n\n\n<p><strong>Prerequisite Materials:<\/strong><\/p>\n\n\n\n<ul class=\"wp-block-list\">\n<li>A laptop with R and RStudio installed<\/li>\n\n\n\n<li>The following R packages:\n<ul class=\"wp-block-list\">\n<li>devtools<\/li>\n\n\n\n<li>testthat<\/li>\n\n\n\n<li>roxygen2<\/li>\n\n\n\n<li>pkgdown<\/li>\n<\/ul>\n<\/li>\n\n\n\n<li>A GitHub account (optional)<\/li>\n<\/ul>\n\n\n\n<h4 class=\"wp-block-heading\">Workshops in the afternoon (from 13:30-16:00)<\/h4>\n\n\n\n<h5 class=\"wp-block-heading\">6) Power analyses through simulations: Annoying and time-consuming, but probably our only real shot. Here\u2019s how you do it!<\/h5>\n\n\n\n<p>Organizer: Bj\u00f6rn J\u00f6rges (York University)<\/p>\n\n\n\n<p>In response to the replication crisis, calls have been made to increase statistical power in psychological studies. Traditional tools for power analyses such as G*Power can accommodate simpler statistical tests like t tests or ANOVAs but fall short for more complex study designs and data structures. And, for better or for worse, such complexities are the norm in Cognitive Psychology, where many participants tend to repeat many trials in many different conditions. The only principled way to approach power analyses in this context is through simulations. This two-hour, hands-on workshop will give a short introduction to the motivation behind power analyses and then walk its participants through the process of setting up one such power analysis in R. This includes specifying numerical predictions, simulating the expected data including variability, specifying the statistical analysis based on Linear Mixed Modelling and running an appropriate number of repetitions of this simulation. We will use two examples, one reaction time task and one psychophysical two-alternative forced-choice task to cover two common use cases with complex data structures. Participants will need a recent installation of R (preferably 4.4.2) and RStudio and they will receive access to an open repository with all code and data used in this workshop.<\/p>\n\n\n\n<h5 class=\"wp-block-heading\">7) MLDS and MLCM: Two scaling methods to study stimulus appearance<\/h5>\n\n\n\n<p>Organizer: Guillermo Aguilar (Technische Universit\u00e4t Berlin)<\/p>\n\n\n\n<p>Maximum Likelihood Difference Scaling (MLDS) and Maximum Likelihood Conjoint Measurement (MLCM) are two methods used to estimate perceptual scales (Knoblauch &amp; Maloney, 2012). These scales reflect stimulus appearance and characterize the mapping of stimulus dimensions to a perceptual dimension of interest. They can also serve as a basis for comparing computational models of the visual system. In this hands-on tutorial, you will learn how to design a typical MLDS\/MLCM experiment and estimate scales using the collected data (in the R programming language). We will also cover the underlying assumptions of the method, how and when these assumptions can be experimentally tested, and provide general recommendations to avoid common pitfalls encountered in practice.<\/p>\n\n\n\n<p>Knowledge on R programming is not strictly required, but attendees should have basic programming skills.<\/p>\n\n\n\n<h5 class=\"wp-block-heading\">8) WaveSpace: A modular python tool for simulating and analyzing cortical traveling waves<\/h5>\n\n\n\n<p>Organizer: Kirsten Petras (Universit\u00e9 Paris Cit\u00e9)<\/p>\n\n\n\n<p>Oscillatory cortical activity has been found to smoothly propagate within and across cortical regions. Finding and characterizing those spatio-temporally consistent traveling waves in multi-channel data recorded with invasive as well as non-invasive techniques requires multiple consecutive, but sometimes interchangeable processing and analysis steps. The current literature on cortical traveling waves lacks a clear consensus of which methods can and should be applied in specific situations.<\/p>\n\n\n\n<p>In this tutorial, we introduce WaveSpace (https:\/\/github.com\/DugueLab\/WaveSpace), a modular simulation and analysis tool in Python implementing a range of different pipelines to find, describe and statistically evaluate traveling waves bundled with a simulation module to generate synthetic data with diverse wave dynamics in realistic background activity. These features allow researchers to generate benchmark analyses tailored to their experimental paradigm and model experimental outcomes in silico.<\/p>\n\n\n\n<p>Attendees will learn to simulate and analyze oscillatory traveling waves and optimize analytical pipelines.<\/p>\n\n\n\n<p>Simulations and example data will be provided, but attendees are also welcome to bring their own data (anything that can be converted into a numpy array of channels x timepoints and has spatial descriptors of sensor positions in either 2D or 3D space will do).<\/p>\n\n\n\n<h5 class=\"wp-block-heading\">9) Principles of working in interdisciplinary teams<\/h5>\n\n\n\n<p>Organizer: Robin Welsch (Aalto University)<\/p>\n\n\n\n<p>This workshop introduces the foundational principles of effective interdisciplinary collaboration, specifically at the intersection of visual perception and computer science. In today\u2019s research landscape, breakthroughs often arise when experts from diverse fields come together. This session will focus on collaborating between disciplines to integrate computational techniques\u2014such as machine learning, data analytics, mixed reality, and computer vision\u2014with experimental approaches in visual perception research but also how to build bridges to other sciences.<\/p>\n\n\n\n<p>The workshop is designed for researchers who want to learn how to navigate the challenges of interdisciplinary teamwork. We will explore fundamental strategies for establishing a common language between disciplines, aligning different methodologies, and overcoming communication barriers that impede collaboration.&nbsp;<\/p>\n\n\n\n<p>During the practical exercise, small groups will simulate the process of forming an interdisciplinary team, identifying complementary skills, and outlining a project that integrates computational methods with perceptual science. This hands-on approach highlights potential obstacles and demonstrates practical solutions for fostering innovative research partnerships.<\/p>\n\n\n\n<p>No prior experience in interdisciplinary projects is required. Participants should bring a laptop to engage fully in the collaborative exercise. This workshop is an ideal opportunity for anyone looking to build or enhance research teams from an interdisciplinary angle to drive novel insights in visual perception.<\/p>\n\n\n\n<h5 class=\"wp-block-heading\">10) Hands-On Eye Tracking with Neon: Advanced Coding Workflows for Experiment Control and Analysis<\/h5>\n\n\n\n<p>Organizer: Pupil Labs GmbH<\/p>\n\n\n\n<p>Curious about taking eye tracking beyond the basics? Get hands-on with our Neon eye tracking system in this practical workshop, focusing on coding workflows for experiment control, data processing and analysis.<\/p>\n\n\n\n<p>You\u2019ll learn how to use Neon\u2019s real-time Python API to stream and visualize raw data as it\u2019s captured\u2014monitoring wearer behavior and controlling a real-world experiment programmatically.<\/p>\n\n\n\n<p>We\u2019ll then guide you through loading and analyzing recorded data using our open-source Python tools, giving you direct experience with Neon\u2019s flexible, open data formats and rich developer ecosystem.<\/p>\n\n\n\n<p>No prior experience with Neon is required\u2014just bring your curiosity and a laptop, and be ready to try your hand at some coding with our guidance!<\/p>\n\n\n\n<p>By the end of this workshop, you\u2019ll see how to move beyond graphical user interfaces and unlock advanced, customizable eye tracking in your own research.<\/p>\n<\/details>\n<\/div>\n<\/div>\n\n\n\n<p><\/p>\n","protected":false},"excerpt":{"rendered":"<p>The ECVP is an annual international conference that aims to provide a forum for the presentation and discussion of new developments in the scientific study of visual perception. Empirical, theoretical, and applied perspectives from the disciplines of psychology, neuroscience, and cognitive science are all welcome and encouraged. Since 1978, ECVP has been one of the [&hellip;]<\/p>\n","protected":false},"author":1,"featured_media":0,"parent":0,"menu_order":0,"comment_status":"closed","ping_status":"closed","template":"","meta":{"footnotes":""},"class_list":["post-1276","page","type-page","status-publish","hentry"],"_links":{"self":[{"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/pages\/1276","targetHints":{"allow":["GET"]}}],"collection":[{"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/pages"}],"about":[{"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/types\/page"}],"author":[{"embeddable":true,"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/users\/1"}],"replies":[{"embeddable":true,"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/comments?post=1276"}],"version-history":[{"count":379,"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/pages\/1276\/revisions"}],"predecessor-version":[{"id":5791,"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/pages\/1276\/revisions\/5791"}],"wp:attachment":[{"href":"https:\/\/ecvp2025.uni-mainz.de\/wp-json\/wp\/v2\/media?parent=1276"}],"curies":[{"name":"wp","href":"https:\/\/api.w.org\/{rel}","templated":true}]}}